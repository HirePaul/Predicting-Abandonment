{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff164c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379c673b-403a-4318-afa9-1d17c6b58fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_list = pd.read_excel('project_5000Up.xlsx')\n",
    "project_list = project_list['Project URL'].tolist()\n",
    "project_list = project_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad8d079-62eb-475d-b170-b0aa445c0c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headless mode for Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--disable-renderer-backgrounding\")\n",
    "chrome_options.add_argument(\"--disable-background-timer-throttling\")\n",
    "chrome_options.add_argument(\"--disable-backgrounding-occluded-windows\")\n",
    "chrome_options.add_argument(\"--disable-client-side-phishing-detection\")\n",
    "chrome_options.add_argument(\"--disable-crash-reporter\")\n",
    "chrome_options.add_argument(\"--disable-oopr-debug-crash-dump\")\n",
    "chrome_options.add_argument(\"--no-crash-upload\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-low-res-tiling\")\n",
    "chrome_options.add_argument(\"--log-level=3\")\n",
    "chrome_options.add_argument(\"--silent\")\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "\n",
    "#proxy_server_url = \"104.251.224.95\"\n",
    "#chrome_options.add_argument(f'--proxy-server={proxy_server_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d06d022-38d0-49a6-b2bd-2ba9d7e3df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_prs(project_url, driver):\n",
    "    project = project_url[19:]\n",
    "    # Pull Requests\n",
    "    pull_url = project_url + \"/pulls\"\n",
    "    \n",
    "    for i in range(0,10):\n",
    "        driver.get(pull_url)\n",
    "        # Wait for the document to be in 'complete' state\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        \n",
    "        open_prs = soup.find(href=f\"/{project}/pulls?q=is%3Aopen+is%3Apr\")\n",
    "        close_prs = soup.find(href=f\"/{project}/pulls?q=is%3Apr+is%3Aclosed\")\n",
    "        if open_prs != None and close_prs != None:\n",
    "            open_prs = open_prs.text.split()[0]\n",
    "            close_prs = close_prs.text.split()[0]\n",
    "            print(f\"{project_url}: {open_prs} open_prs and {close_prs} close_prs\")\n",
    "            return [open_prs, close_prs]\n",
    "        else:\n",
    "            time.sleep(10)\n",
    "    print(f\"{project_url}: open_prs and close_prs not found\")\n",
    "    return [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85b2537-685a-455a-ab35-9a074a7b8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_owner(project_url, driver):\n",
    "    project = project_url[19:]\n",
    "    creator = project.split('/')[0]\n",
    "    \n",
    "    # Verified Repo Owner\n",
    "    owner_url = f\"https://github.com/{creator}\"\n",
    "    driver.get(owner_url)\n",
    "\n",
    "    # Wait for the document to be in 'complete' state\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "    )    \n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    verified = soup.find('summary', {'title': 'Label: Verified'})\n",
    "    if verified != None:\n",
    "        verified = verified.text.split()[0]\n",
    "        \n",
    "    print(f\"{project_url} owner status: {verified}\")\n",
    "\n",
    "    # Number of Owner Followers\n",
    "    followers = soup.find('a', class_='Link--secondary no-underline no-wrap')\n",
    "    if followers != None:\n",
    "        followers = followers.text.split()[0]\n",
    "    print(f\"{project_url} followers: {followers}\")\n",
    "\n",
    "    # Number of Owner Members\n",
    "    members = soup.find('span', class_='Counter js-profile-member-count')\n",
    "\n",
    "    if members != None:\n",
    "        while members.text == \"\":\n",
    "            driver.get(owner_url)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "            )    \n",
    "        \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            time.sleep(5)\n",
    "            members = soup.find('span', class_='Counter js-profile-member-count')\n",
    "        members = members.text.split()[0]\n",
    "        \n",
    "    print(f\"{project_url} members: {members}\")\n",
    "\n",
    "    # Number of Other Repositories by Owner\n",
    "    repositories = soup.find('span', class_='Counter js-profile-repository-count')\n",
    "    if repositories == None:\n",
    "        repositories = soup.find_all('span', class_='Counter')[0]\n",
    "        \n",
    "    if repositories != None:\n",
    "        repositories = repositories.text.split()[0]\n",
    "    print(f\"{project_url} repositories: {repositories}\")\n",
    "    \n",
    "    return [verified, followers, members, repositories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f343d1cf-46fc-4b88-b7fb-6588a3e813d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_insight(project_url, driver):\n",
    "    project = project_url[19:]\n",
    "    creator = project.split('/')[0]\n",
    "    \n",
    "    # Active prs and active issues\n",
    "    insight_url = f\"{project_url}/pulse\"\n",
    "    driver.get(insight_url)\n",
    "\n",
    "    # Wait for the document to be in 'complete' state\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "    )    \n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    active = soup.find_all('div', class_='mt-2')\n",
    "    active_prs = active[0]\n",
    "    active_issues = active[1]\n",
    "\n",
    "    if active_prs != None:\n",
    "        active_prs = active_prs.text.split()[0]\n",
    "\n",
    "    if active_issues != None:\n",
    "        active_issues = active_issues.text.split()[0]\n",
    "\n",
    "    print(f\"{project_url}: {active_prs} Active pull requests, {active_issues} Active issues\")\n",
    "    return [active_prs, active_issues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5861545e-cb07-4106-940c-96d8cb8d9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_issues(project_url, driver):\n",
    "    project = project_url[19:]\n",
    "    # Issues\n",
    "    issue_url = project_url + \"/issues\"\n",
    "\n",
    "    for i in range(0,10):\n",
    "        driver.get(issue_url)\n",
    "    \n",
    "        # Wait for the document to be in 'complete' state\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "        open_issues = soup.find(href=f\"/{project}/issues?q=is%3Aopen+is%3Aissue\") \n",
    "        closed_issues = soup.find(href=f\"/{project}/issues?q=is%3Aissue+is%3Aclosed\")\n",
    "\n",
    "        num_labels = soup.find(href=f\"/{project}/labels\")\n",
    "        num_milestones = soup.find(href=f\"/{project}/milestones\")\n",
    "        \n",
    "        if open_issues != None:\n",
    "            open_issues = open_issues.text.split()[0]\n",
    "            closed_issues = closed_issues.text.split()[0]\n",
    "            num_labels = num_labels.find(\"span\").text\n",
    "            num_milestones = num_milestones.find(\"span\").text\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(10)\n",
    "\n",
    "    if type(num_labels) != int:\n",
    "        # labels\n",
    "        driver.get(project_url + '/labels')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        num_labels = soup.find('span', class_='js-labels-count')\n",
    "        num_labels = num_labels.text.split()[0]\n",
    "\n",
    "        # milestones\n",
    "        driver.get(project_url + '/milestones')\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        num_milestones = soup.find('a', class_='btn-link selected').text.split()[0]\n",
    "\n",
    "    print(f\"{project_url}, Open issues: {open_issues}, Closed issues: {closed_issues}, Labels: {num_labels}, Milestones: {num_milestones}\")\n",
    "    return [open_issues, closed_issues, num_labels, num_milestones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f28d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### IDEA: Merge Two Pandas DataFrames using Outer Join Merge ###########\n",
    "# Potential Scrapes:\n",
    "# ../issues => Open/closed issues, # of labels, # of milestones\n",
    "# ../pulls\n",
    "# ../actions => # of workflow runs\n",
    "# ../pulse => Active pr and Active issues\n",
    "# ../network/dependencies\n",
    "# Sponsered\n",
    "# Watches\n",
    "# \n",
    "# Ones in Bash Script:\n",
    "# Commits\n",
    "# Tags/Releases\n",
    "# Branches\n",
    "# Languages\n",
    "# Contributors\n",
    "\n",
    "def scrape_page(project_url, driver):\n",
    "\n",
    "    project_features = []\n",
    "    \n",
    "    # Get the OWNER/REPO\n",
    "    project = project_url[19:]\n",
    "\n",
    "    # Set up Web Driver\n",
    "    driver.get(project_url)\n",
    "\n",
    "    # Get number of watches and sponsors\n",
    "    # Wait for the document to be in 'complete' state\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "    )\n",
    "    \n",
    "    # Parse HTML\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "    num_watches = soup.find(href=f\"/{project}/watchers\").find(\"strong\").text\n",
    "    \n",
    "    creator = project.split('/')[0]\n",
    "    sponsored = \"Yes\" if soup.find(href=f\"/sponsors/{creator}\") != None else \"No\"\n",
    "    \n",
    "    if sponsored == \"Yes\":\n",
    "        #for i in range(0,10):\n",
    "        driver.get(f\"https://github.com/sponsors/{creator}\")\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "        )\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        \n",
    "        current_sponsors = soup.find(lambda tag: tag.name == 'h4' and 'Current sponsors' in tag.get_text())\n",
    "        past_sponsors = soup.find(lambda tag: tag.name == 'h4' and 'Past sponsors' in tag.get_text())\n",
    "\n",
    "        if current_sponsors == None:\n",
    "            current_sponsors = soup.find('p', class_='f3-light color-fg-muted mb-3')\n",
    "            current_sponsors = current_sponsors.text.split()[0]\n",
    "            past_sponsors = 0\n",
    "        else:\n",
    "            current_sponsors = current_sponsors.text.split()[2]\n",
    "            past_sponsors = past_sponsors.text.split()[2]\n",
    "\n",
    "    else:\n",
    "        current_sponsors = 0\n",
    "        past_sponsors = 0\n",
    "\n",
    "    print(f\"{project_url}: {current_sponsors} Current sponsors, {past_sponsors} Past sponsors\")\n",
    "    project_features.append(sponsored)\n",
    "    project_features.append(current_sponsors)\n",
    "    project_features.append(past_sponsors)\n",
    "    project_features.append(num_watches)\n",
    "\n",
    "    # Number of Workflow Runs\n",
    "    workflow_url = project_url + \"/actions\"\n",
    "    driver.get(workflow_url)\n",
    "\n",
    "    # Wait for the document to be in 'complete' state\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "    )\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    workflow = soup.find(lambda tag: tag.name == 'strong' and 'workflow runs' in tag.get_text())\n",
    "    if workflow != None:\n",
    "        workflow = workflow.text.split()[0]\n",
    "\n",
    "    project_features.append(workflow)\n",
    "    \n",
    "    # Number of Dependent Repos\n",
    "    dependent_url = project_url + \"/network/dependents\"\n",
    "    driver.get(dependent_url)\n",
    "\n",
    "    # Wait for the document to be in 'complete' state\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.TAG_NAME, 'body'))\n",
    "    )\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    dependents = soup.find('a', class_='btn-link selected')\n",
    "    if dependents != None:\n",
    "        dependents = dependents.text.split()[0]\n",
    "    \n",
    "    project_features.append(dependents)\n",
    "    print(f\"{project_url} {dependents} dependents, {workflow} workflows\")\n",
    "\n",
    "    return project_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4a65f2-5a5b-45cc-8b75-12387ee3736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_project(project_url):\n",
    "    project = [project_url]\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    prs = scrape_prs(project_url, driver)\n",
    "    time.sleep(1)\n",
    "    owner = scrape_owner(project_url, driver)\n",
    "    time.sleep(1)\n",
    "    insight = scrape_insight(project_url, driver)\n",
    "    time.sleep(1)\n",
    "    issues = scrape_issues(project_url, driver)\n",
    "    time.sleep(1)\n",
    "    page = scrape_page(project_url, driver)\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    project = project + prs + owner + insight + issues + page\n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aaebd9e-4a0d-4289-8783-2e9922fa32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_project_list(project_list):\n",
    "    projects = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as p:\n",
    "        features = p.map(scrape_project, project_list)\n",
    "        for f in features:\n",
    "            projects.append(f)\n",
    "    return projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b42aced-8047-466d-b509-c82b935783a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'scrape_project_list' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'scrape_project_list' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'scrape_project_list' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'scrape_project_list' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-7:\n",
      "Process SpawnPoolWorker-6:\n",
      "Process SpawnPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/dlu/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sublists \u001b[38;5;241m=\u001b[39m [project_list[i:i\u001b[38;5;241m+\u001b[39msub_len] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, list_len, sub_len)]\n\u001b[1;32m      5\u001b[0m pool \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(scrape_project_list, sublists)\n\u001b[1;32m      7\u001b[0m projects \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    list_len = len(project_list)\n",
    "    sub_len = list_len // 3\n",
    "    sublists = [project_list[i:i+sub_len] for i in range(0, list_len, sub_len)]\n",
    "    pool = multiprocessing.Pool(processes=3)\n",
    "    results = pool.map(scrape_project_list, sublists)\n",
    "    projects = []\n",
    "    for result in results:\n",
    "        projects.extend(result)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"elapsed time is {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a81479",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = pd.DataFrame(projects, columns=['Project URL',\n",
    "                                              'Open Pull Requests',\n",
    "                                              'Closed Pull Requests',\n",
    "                                              'Verified Owner',\n",
    "                                              'Followers of Owner',\n",
    "                                              'Members of Owner',\n",
    "                                              'Repos of Owner',\n",
    "                                              'Active Pull Requests', \n",
    "                                              'Active Issues',\n",
    "                                              'Open Issues',\n",
    "                                              'Closed Issues',\n",
    "                                              'Number of Labels',\n",
    "                                              'Number of Milestones',\n",
    "                                              'Sponsored',\n",
    "                                              'Current Sponsors',\n",
    "                                              'Past Sponsors',\n",
    "                                              'Number of Watches',\n",
    "                                              'Number of Workflow Runs',\n",
    "                                              'Number of Dependents'\n",
    "                                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbe4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664715f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3cfd35",
   "metadata": {},
   "source": [
    "try:\n",
    "    with pd.ExcelWriter(\n",
    "        \"project_HTMLfeatures.xlsx\",\n",
    "        mode=\"a\",\n",
    "        engine=\"openpyxl\",\n",
    "        if_sheet_exists=\"overlay\",\n",
    "    ) as writer:\n",
    "         projects_df.to_excel(writer,sheet_name=\"Sheet1\", startrow=writer.sheets[\"Sheet1\"].max_row, index = False,header= False)\n",
    "except FileNotFoundError:\n",
    "    projects_df.to_excel(\"project_HTMLfeatures.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
